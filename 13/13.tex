%-------------------------------------------------------------------------------------------
\documentclass[aspectratio=169,UTF8,11pt]{ctexbeamer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{colortbl}
\usepackage{color}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
%\usepackage{babel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mode<presentation> {
\usetheme{Madrid}
%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
\setbeamertemplate{footline}[frame number] % To replace the footer line in all slides with a simple slide count uncomment this line
\setbeamercolor{page number in head/foot}{fg=blue}
\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% User Defined Block %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{setspace}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{aa}{RGB}{34,139,34}
\definecolor{lightblue}{rgb}{0,0.85,0.9}
\definecolor{darkblue}{rgb}{0,0.7,1}

\definecolor{hanblue}{rgb}{0.27, 0.42, 0.81}
\definecolor{indiagreen}{rgb}{0.07, 0.53, 0.03}
\definecolor{indianred}{rgb}{0.8, 0.36, 0.36}
\definecolor{indianyellow}{rgb}{0.89, 0.66, 0.34}
\definecolor{babypink}{rgb}{0.96, 0.76, 0.76}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\setbeamerfont{block title}{size=\normalsize}
\setbeamerfont{block body}{size=\small}

\newenvironment<>{blueblock}[1]{%
  \setbeamercolor{block title}{fg=white,bg=hanblue}%
  \begin{block}#2{#1}}{\end{block}}

\newenvironment<>{greenblock}[1]{%
  \setstretch{1.3}\setbeamercolor{block title}{fg=white,bg=indiagreen}%
  \begin{block}#2{#1}}{\end{block}}

\newenvironment<>{redblock}[1]{%
  \setstretch{1.3}\setbeamercolor{block title}{fg=white,bg=indianred}%
  \begin{block}#2{#1}}{\end{block}}

\newenvironment<>{yellowblock}[1]{%
  \setstretch{1.3}\setbeamercolor{block title}{fg=white,bg=indianyellow}%
  \begin{block}#2{#1}}{\end{block}}

%----------------------------------------------------------------------------------------
%	PACKAGES
%----------------------------------------------------------------------------------------
\usepackage{graphicx} % Allows including images
%\usepackage{tikz}
%\usetikzlibrary{shapes.geometric, arrows}
\usepackage{listings}
\lstset{language=C++,
    columns=flexible,
   % basicstyle=\scriptsize\ttfamily,                                      % 设定代码字体、大小4
    basicstyle=\footnotesize\ttfamily,
    %numbers=left,xleftmargin=2em,framexleftmargin=2em,                   % 在左侧显示行号
    %numberstyle=\color{darkgray},                                        % 设定行号格式
    keywordstyle=\color{blue},                                            % 设定关键字格式
    commentstyle=\color{ao(english)},                                     % 设置代码注释的格式
    stringstyle=\color{brown},                                            % 设置字符串格式
    %showstringspaces=false,                                              % 控制是否显示空格
	%frame=lines,                                                         % 控制外框
    breaklines,                                                           % 控制是否折行
    postbreak=\space,                                                     % 控制折行后显示的标识字符
    breakindent=5pt,                                                      % 控制折行后缩进数量
    emph={size\_t,array,deque,list,map,queue,set,stack,vector,string,pair,tuple}, % 非内置类型
    emphstyle={\color{teal}},
    escapeinside={(*@}{@*)},
}
%---------------------------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[\textit{智能优化与最优化方法}]{13 Large-scale Global Optimization}
\author[李长河]{李长河} % Your name
\institute[CUG] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
中国地质大学（武汉）自动化学院\\ % Your institution for the title page
\medskip
\textit{lichanghe@cug.edu.cn} % Your email address
}
\date{} % Date, can be changed to a custom date
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usefonttheme[onlymath]{serif}
\begin{document}
\maketitle
\begin{frame}[noframenumbering]           %beamer里重要的概念，每个frame定义一张page
\centering
{\large 李长河 \vspace{0.5cm} \\自动化学院710 \vspace{0.5cm}\\ lichanghe@cug.edu.cn}
\end{frame}

%-----------------------------------------------------------


\addtocounter{framenumber}{-1}
%---------------------------------------------------------------------------------------------

\begin{frame}
  {Large-scale Global Optimization}
  \begin{block}{Complex Optimization Problems}
    \begin{itemize}
    \item One of the manifestations of complexity is that \alert{the increase in the number of variables} causes the \alert{dimensionality disaster}.
    \item Large-scale global optimization (LSGO) problems usually contains more than 1000 variables.
    \end{itemize}
  \end{block}
  \pause
  \begin{greenblock}{Application Examples}
    \begin{itemize}
      \item The large-scale power system design
      \item Vehicle routing problems
      \item Genetic identification
      \item Inverse chemical kinetics problem
    \end{itemize}
  \end{greenblock}
\end{frame}

\begin{frame}{Contents}
	\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13.1 Large-scale Global Optimization Problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{13.1.1 Definition}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Large-scale Global Optimization Problems\small{-Definition}}
  \begin{block}{Complex Optimization Problems}
    \begin{itemize}
    \item To effectively solve the large-scale global optimization problem, we normally do not search in the original search space.
    \item According to the correlation between variables, the problem can be \alert{separable}, \alert{partially separable} or \alert{non-separable}.
    \end{itemize}
  \end{block}

  \only<1>{
  \begin{greenblock}
    {Fully separable function}
    $f(\bm{x})$ is a {\bf fully separable} function iff (if and only if) \index{F!fully separable}
    \begin{align}
    \min / \max f(\bm{x})=\left(f\left(x_{1}\right), f\left(x_{2}\right),\cdots,  f\left(x_{n}\right)\right) \tag{1}\label{eq13-1}
    \end{align}
    where $\bm{x} = \left(x_{1}, x_{2}, \cdots, x_{n}\right)$ is a D-dimensional decision vector.
  \end{greenblock}
  }

  \only<2>{
  \begin{greenblock}
    {Partially separable function}
    $f(\bm{x})$ is a {\bf partially separable} function with $m$ independent subcomponents iff \index{P!partially separable}
    \begin{align}
    \min / \max f(\bm{x})=\left(f\left(\bm{x_{1}}, \cdots\right),\cdots,  f\left(\cdots,\bm{x_{m}}\right)\right) \tag{2}\label{eq13-2}
    \end{align}
    where $\bm{x_{1}}, \cdots, \bm{x_{m}}$ are disjoint sub-vectors of $\bm{x}$ and $2 \leq m<n$.
  \end{greenblock}
  }

  \only<3>{
    \begin{greenblock}
      {Fully non-separable function}
      $f(\bm{x})$ is a {\bf fully non-separable} function, if every pair of its decision variables interacts with each other.\index{F!fully non-separable}
    \end{greenblock}
  }

  \only<4>{
    \begin{greenblock}
      {Partially additively separable function}
      $f(\bm{x})$ is a {\bf partially additively separable} function iff\index{P!partially additively separable}
      \begin{align}
      f(\bm{x})=\sum_{i=1}^{m} f_{i}\left(\bm{x_{i}}\right) \tag{3}\label{eq13-1-4}
      \end{align}
      where $\bm{x_{i}}$ are mutually exclusive decision vectors of $f_{i}$ ; $m$ is the number of separable subcomponents.
    \end{greenblock}
  }
  
\end{frame}


\subsection{13.1.2 Difficulties}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Large-scale Global Optimization Problems\small{-Difficulties}}
  \begin{block}
    {Difficulties}
    {\bf Non-separable} and {\bf overlapping} functions are the most difficult to solve:
    \begin{itemize}
      \item The search space generally \alert{increases exponentially}.
      \item The algorithm is easy to \alert{fall into local optima}.
      \item The objective function generally has the characteristics of \alert{nonlinear}, \alert{non-convex}, \alert{multi-modal} and \alert{non-differentiable}.
      \item Variables are \alert{partially separable} or \alert{completely non-separable}.
    \end{itemize}
    {\bf It is also challenging for EAs to explore the entire search space effectively.}
  \end{block}
  \pause
  \begin{greenblock}{Two branches of metaheuristics}
    \begin{itemize}
      \item Space decomposition methods (co-evolution).
      \item Non-decomposition-based methods.
    \end{itemize}
  \end{greenblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13.2 Co-evolution Methods}
\begin{frame}
  {Co-evolution Methods}
  \begin{block}{Methods Mechanism}
    \begin{itemize}
      \item Decompose the large-scale global optimization problem into several \alert{low-dimensional sub-problems}.
      \item Use the \alert{divide-and-conquer method} to solve the large-scale optimization problem.
    \end{itemize}
  \end{block}
  \pause
  \begin{redblock}{Challenge}
    \begin{itemize}
      \item The interaction between variables in indivisible problems.
      \item It will have a great influence on the optimization efficiency of the algorithm.
    \end{itemize}
  \end{redblock}
  \pause
  \begin{yellowblock}{Solution}
    Most decomposition methods try to \alert{identify} the interacting variables and \alert{assign} them to the same in the sub-question.
  \end{yellowblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{13.2.1 Co-operative Co-evolution}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Co-operative Co-evolution}}  
  \begin{block}{Co-operative co-evolutionary (CC) approach}
    Potter and De Jong proposed the co-operative co-evolutionary (CC) approach in 1994 to improve the performance of GAs.
  \end{block}
  \pause
  \begin{greenblock}{The framework of classic CC algorithm}
    \begin{itemize}
      \item Problem decomposition.
      \item Subcomponent optimization.
      \item Cooperative combination.
    \end{itemize}
  \end{greenblock}
  \pause
  \begin{yellowblock}{Categories}
    Based on different grouping strategies, the co-evolution algorithms used to solve LSGO problems are mainly divided into two categories: \alert{static grouping methods} and \alert{dynamic grouping methods}.
  \end{yellowblock}
\end{frame}

\subsection{13.2.2 Static Grouping}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Static Grouping}}
  \begin{block}{Limitations of CC algorithm}
    \begin{itemize}
      \item Being tested only for the maximum dimension of 30.
      \item Being less effective than classical GAs in non-separable problems.
    \end{itemize}
  \end{block}
  \pause
  \begin{greenblock}{Solutions}
    \begin{itemize}
      \item Liu et al. combined CC framework with FEP (fast evolutionary programming) to solve the problem of 100–1000 dimensional continuous optimization.
      \item Scholars tried to combine the CC idea with swarm intelligence-based algorithms,
      such as CPSO-SK and CPSO-HK.
    \end{itemize}
  \end{greenblock}
  \pause
  \begin{yellowblock}{Limitations of Static grouping}
    Only effective in low-dimensional problems.
  \end{yellowblock}
\end{frame}


\subsection{13.2.3 Dynamic Grouping}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{block}{Relationships between static and dynamic grouping}
    \begin{itemize}
      \item They both try to detect the interacting relationship between variables and assign the variables that interacts each other to the same sub-component.
      \item In static grouping, the number of sub-components $(k)$ is fixed, while in dynamic grouping, the structure of sub-components can be dynamically adjusted.
    \end{itemize}
  \end{block}
  \pause
  \begin{greenblock}{Categories}
    Dynamic grouping methods can be divided into two categories based on the substitution of variables in group components: \alert{random dynamic grouping} and \alert{learning-based dynamic grouping}.
  \end{greenblock}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{block}{}
		1. Random dynamic grouping
	\end{block}
  \begin{block}{DECC-G algorithm}
    Yang et al. proposed a \alert{DE-based co-evolution algorithm (DECC-G) using random grouping strategy} to attempt to solve the non-decomposing LSGO problem of 500–1000 dimensions.
    \begin{itemize}
      \item Decomposed an n-dimensional target vector into \alert{multiple low-dimensional subcomponents}, each of which is \alert{evolved by a self-adaptive DE} with a neighborhood search algorithm.
      \item Proposed an \alert{adaptive weighting framework} to further improve solutions, which assigns a weight to each of the subcomponents after each cycle.
      \item An optimization algorithm is used to \alert{optimize these weights}, and the dimension of the optimization problem is much lower than before.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{greenblock}{The framework of the DECC-G algorithm}
    \begin{algorithm}[H]
      \caption{DECC-G algorithm}
      \While{$termination \, criterion \,is\, not\, fulfilled$}{
      Set $i=0$\;
      The $n$-dimensional object vector is randomly divided into $m$ $s$-dimensional subcomponents\;
      \While{$i<m$}{
      $i++$\;
      Evolve the $i$th subcomponent with a certain EA\;
      }
      Assign a weight vector for each subcomponent\;
      Optimize them via a certain EA\;
      }
    \end{algorithm}
  \end{greenblock}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{block}{Multilevel CC algorithm (MLCC) algorithm}
    To improve the performance of DECC-G, Yang et al. proposed \alert{a new multilevel CC algorithm (MLCC)}.
    \begin{itemize}
      \item A \alert{decomposer pool} is used in MLCC to reduce the impact of the group size on the objective function evolution.
      \item Each decomposer specifies a group size, select the current decomposer for grouping objective vector based on \alert{the performance of decomposers} in evolution process, and updates the decomposer.
    \end{itemize}
  \end{block}
  \begin{blueblock}{The performance of MLCC algorithm}
    This method is more applicable to most real-word problems. 
  \end{blueblock}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{block}{}
		2. Learning-based Dynamic Grouping\\
    Decomposing problems with interactions between variables requires \alert{prior knowledge} of the problem.
    \begin{itemize}
      \item In such methods, the identification of interactions between variables is learned by the \alert{characteristic experience of the problems} obtained before or during the optimization process.
      \item The purpose of this type methods is to increase the chance of of interacting variables being assigned to the same subcomponent.
    \end{itemize}
	\end{block}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{block}{CCEA-AVP algorithm}
    Ray and Yao proposed a co-operative co-evolutionary algorithm based on correlation matrix based adaptive variable partitioning (CCEA-AVP).
  \end{block}
  \begin{greenblock}{The framework of CCEA-AVP algorithm}
    \begin{itemize}
      \item In the first several cycles of running, a subcomponent contains all the variables, so the evolution process is similar to the standard EA.
      \item In the following cycle, the decision variables are divided into several sub-components according to the value of the \alert{correlation between decision variables}.
      \item Thus, the variables whose correlation coefficient is greater than a certain threshold are placed in the \alert{same sub-component}.
    \end{itemize}
  \end{greenblock}
\end{frame}

\begin{frame}
  {Co-evolution Methods\small{-Dynamic Grouping}}
  \begin{greenblock}{}
    Some other methods inspired by CC algorithm
  \end{greenblock}
  \begin{block}{CCEA-AVP with an adaptive grouping method}
    CCEA-AVP with an adaptive grouping method is inspired by CCEA-AVP.
    \begin{itemize}
      \item The method based on correlation coefficient has a \alert{large amount of calculations}.
      \item It cannot identify the \alert{nonlinear dependence between variables}.
    \end{itemize}
  \end{block}
  \begin{block}{CBCC algorithm}
    A co-evolutionary algorithm according to the \alert{contribution of each sub-component} called contribution based cooperative co-evolution(CBCC).
    \begin{itemize}
      \item Being proposed to \alert{allocate computing resources}.
      \item Can \alert{significantly reduce the computational time} if there is an imbalance among the separable and non-separable parts of the fitness value in the LSGO problem.
    \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}
%   {Co-evolution Methods\small{-Dynamic Grouping}}
%   \only<1>{
%     \begin{block}{}
%       To decompose an LSGO problem, we first need to know if there is any interaction between the variables.
%     \end{block}
%     \begin{block}{Identifying interactions between variables}
%       Suppose:
%       \begin{itemize}
%         \item ``$\bm{best}$'' is the best solution currently obtained.
%         \item ``$\bm{new}$'' represents the best individual obtained by the CC optimizer for dimension $i$.
%         \item ``$\bm{rand}$'' represents an individual randomly selected in the population.
%       \end{itemize}
%       According to the following principle, two new individuals will be generated from the above three vectors
%       \begin{align}
%         x_{j}=
%         \begin{cases}
%         {n e w_{i}} & {\text { if } j=i} \\
%         {b e s t_{j}} & {\text { otherwise }}
%         \end{cases} x_{j}'=
%         \begin{cases}
%         {n e w_{i}} & {\text { if } j=i} \\
%         {r a n d_{k}} & {\text { if } j=k} \\
%         {b e s t_{j}} & {\text { otherwise }}
%         \end{cases} \tag{4}\label{eq13-2-1}
%       \end{align}
%     \end{block}
%   }

%   \only<2>{
%     \begin{block}{Identifying interactions between variables}
%       \begin{align}
%         x_{j}=
%         \begin{cases}
%         {n e w_{i}} & {\text { if } j=i} \\
%         {b e s t_{j}} & {\text { otherwise }}
%         \end{cases} x_{j}'=
%         \begin{cases}
%         {n e w_{i}} & {\text { if } j=i} \\
%         {r a n d_{k}} & {\text { if } j=k} \\
%         {b e s t_{j}} & {\text { otherwise }}
%         \end{cases} \tag{4}
%       \end{align}
%       If $f\left(x'\right)$ is better than $f(x)$, the probability of interaction between dimensions $i$ and $k$ increases.
%     \end{block}
%     \begin{greenblock}{CCVIL algorithm}
%       Based on the above ideas, Chen et al. proposed a method called CCVIL (i.e., CC method with variable interaction learning) that can adjust the group size adaptively. This approach consists of two phases: learning and optimization. In the learning phase, the main purpose is to detected \alert{the interaction between variables}.
%     \end{greenblock}
%   }
% \end{frame}

% \begin{frame}
%   {Co-evolution Methods\small{-Dynamic Grouping}}
%   \begin{block}{Theorem 1 Identifying the interaction between two variables}
%     Suppose $f(\bm{x})$ is an additively separable function, $\forall a, b_{1} \neq b_{2}, \delta \in \mathbb{R}, \delta \neq 0$, if the following condition holds, then $x_p$ and $x_q$ are nonseparable
%     \begin{align}
%      \Delta_{\delta, x_{p}}[f](\bm{x})|_{x_{p}=a, x_{q}=b_{1}} \neq \Delta_{ \delta, x_{p}}[f]\left.(\bm{x})\right|_{x_{p}=a, x_{q}=b_{2}} \tag{5}\label{eq13-2-2}
%     \end{align}
%     where $\Delta_{\delta, x_{p}}[f](\bm{x})=f\left(\ldots, x_{p}+\delta, \ldots\right)-f\left(\ldots, x_{p}, \ldots\right)$ is the forward difference of $f$ according to variable $x_p$ with the interval $\delta$.
%   \end{block}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13.3 Non-decomposition-based Methods}
\begin{frame}
  {Non-decomposition-based Methods}
  % \begin{columns}[T]
  %   \column{0.5\textwidth}
    \begin{block}{Non-decomposition-based Methods}
      The non-decomposed method mainly attempts to improve the performance of the standard meta heuristic algorithm to solve the LSGO problem.
    \end{block}
    % \column{0.4\textwidth}
    \begin{greenblock}{Such algorithms focus on:}
      \begin{columns}[T]
        \column{0.5\textwidth}
        \begin{itemize}
          \item Defining new mutations
          \item Selection and crossover operators
          \item Hybridization
          \item Opposition-based learning
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
          \item Designing and using local search
          \item Sampling operators
          \item Variable population size methods
        \end{itemize}
      \end{columns}
    \end{greenblock}
  % \end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{13.3.1 PSO-based Algorithms}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Non-decomposition-based Methods\small{-PSO-based Algorithms}}
  \begin{block}{PSO-based algorithms}
    Unlike standard PSO, a PSO with \alert{velocity modulation} and \alert{restarting strategy} proposed was proposed.
    \begin{itemize}
      \item Velocity modulation controls the directional motion of particles in a finite range.
      \item Restart strategies are used to prevent premature. If the overall change in the standard deviation of the particle fitness in the whole population is very small, the restart strategy is employed.
    \end{itemize}
  \end{block}
  \begin{greenblock}{Incremental particle swarm optimizer with local search (IPSOLS) algorithm}
    Using a tuning-in-the-loop approach to redesign the IPSOLS.
    % \\
    % The redesign process has six stages: selecting the local search method, alteration of calling and controlling the local search method, using vector PSO rules, penalizing bound constraints violation, and fighting stagnation with restarting.
  \end{greenblock}
\end{frame}

\subsection{13.3.2 EDA-based Algorithms}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Non-decomposition-based Methods\small{-EDA-based Algorithms}}
  \begin{block}{EDA-based Algorithms}
    EDA is a random population optimization algorithm based on statistical principles.
  \end{block}
  \begin{greenblock}{LSEDA-GL algorithm}
    \begin{itemize}
      \item Using the Gaussian sampling, the Levy probability distribution and a restart strategy to prevent premature.
    \end{itemize}
  \end{greenblock}
\end{frame}


\subsection{13.3.3 DE-based Algorithms}
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Non-decomposition-based Methods\small{-DE-based Algorithms}}
  \begin{block}{DE-based Algorithms}
    Because the DE algorithm has the characteristics of simple structure and strong robustness, many researchers have improved the DE algorithm and applied it to LSGO problems.
  \end{block}
  \begin{greenblock}{Mutation operation DE/current-to-pbest}
    \begin{itemize}
      \item Optimizing each subcomponent with a random dynamic grouping method and the weight of in the adaptive weighting process.
    \end{itemize}
  \end{greenblock}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13.4 Learning-based Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\begin{frame}
  {Learning-based Methods}
  \begin{block}{}
    Combination optimization problems related to practical engineering are often large-scale problems. Such as the \alert{vehicle routing problem (VRP)} and \alert{bin packing problem (BPP)}.
  \end{block}
  \begin{greenblock}{}
    Algorithms for solving these problems mainly focus on the \alert{exact algorithms} and \alert{heuristic algorithms}.
  \end{greenblock}
  \pause
  \begin{greenblock}{Advantages and disadvantages the methods}
    \begin{itemize}
      \item The exact algorithm: Can get the exact optimal solution of the problem but require \alert{a large amount of computation}, only being suitable for solving \alert{small-scale} combinatorial optimization problems.
      \item The heuristic algorithm: \alert{Fast} but can only get the \alert{approximate optimal solution}.
    \end{itemize}
  \end{greenblock}
  \begin{redblock}{}
    Both types of algorithms require \alert{a lot of expertise or experience} to design special search strategies for different types of problems in order to obtain good search results and ensure computational performance. 
  \end{redblock}
\end{frame}

\begin{frame}
  {Learning-based Methods}
  \begin{block}{}
    With the rise of machine learning, learning methods have been used to solve these difficult optimization problems.
  \end{block}
  \begin{block}{Advantages of machine learning methods}
    \begin{itemize}
      \item The unique \alert{priori knowledge} of the problem can be learned.
      \item More effective \alert{heuristic strategies} can be learned.
    \end{itemize}
  \end{block}
  % \begin{greenblock}{Example: Noah’s Ark Lab of Huawei}
  %   The following is illustrated by the project of Noah’s Ark Lab of Huawei on the integration of vehicle path planning and 3D packing problem.
  % \end{greenblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{13.5 Discussions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \tableofcontents[currentsection,currentsubsection]
\end{frame}

\end{document}
